<<<<<<< HEAD
# 要件定義書 - AI VTuber開発プロジェクト

## プロジェクト概要

**プロジェクト名**: AI VTuber開発プロジェクト  
**目標**: 1枚の絵（yume_image.png）からLive2Dモデルとして動かせるようにし、最終的にTikTok等でライブ配信できるAI VTuberシステムを構築

## 基本要件

### 1. 機能要件

#### 1.1 キャラクター動作
- **入力**: 1枚の画像（yume_image.png）
- **出力**: リアルタイムで動作するキャラクター
- **動作種類**:
  - 感情表現（嬉しい、悲しい、怒り、驚き、愛、興奮）
  - アクション（手振り、ジャンプ、うなずき、首振り、瞬き、スマイル）
- **画像認識による制御**:
  - 現在のキャラクター状態の認識
  - 適切なアニメーション遷移の決定
  - 感情・動作の連続性の維持
  - 自然な動作の実現

#### 1.2 コメント連携
- **入力**: テキストコメント
- **処理**: コメント解析→感情・アクション検出→画像認識→キャラクター動作
- **対応言語**: 日本語
- **キーワード検出**: 感情・アクションキーワードの自動検出
- **画像認識連携**:
  - 現在のキャラクター状態を画像認識で把握
  - コメント内容と現在状態を照合
  - 最適なアニメーション遷移を決定
  - 自然な動作の連続性を保証

#### 1.3 配信機能
- **配信プラットフォーム**: TikTok、YouTube Live
- **配信方式**: リアルタイム配信
- **視聴者インタラクション**: コメント→キャラクター動作

### 2. 非機能要件

#### 2.1 パフォーマンス
- **リアルタイム性**: コメント入力から動作開始まで3秒以内
- **品質**: 高品質な動画生成
- **安定性**: 長時間配信に対応

#### 2.2 可用性
- **稼働時間**: 24時間365日対応
- **エラー処理**: 適切なエラーハンドリング
- **復旧性**: 障害時の自動復旧

#### 2.3 拡張性
- **スケーラビリティ**: 複数キャラクター対応
- **カスタマイズ性**: 新しい感情・アクションの追加
- **API連携**: 外部システムとの連携

## 技術要件

### 3. 使用技術

#### 3.1 AI技術（ハカセアイの記事より）
- **Wan S2V**: 音声から映画品質の動画生成
- **PixAI i2v v3.0**: 高一貫性の画像→動画生成
- **RAIN**: リアルタイムアニメーション生成AI
- **EMO**: 写真/イラストからリアルな表情生成
- **MoCha**: 映画品質の人間動画生成
- **StreamDiffusion**: リアルタイム画像生成パイプライン

#### 3.1.1 画像認識AI（必須）
- **CLIP (Contrastive Language-Image Pre-training)**: 
  - 画像とテキストの意味的関連性を理解
  - キャラクターの表情・動作・感情を正確に認識
  - リアルタイム画像解析による動的アニメーション制御
- **Gemini 2.5 Flash**:
  - マルチモーダル画像理解
  - 高精度な画像内容解析
  - 感情・動作・表情の詳細分析
  - キャラクターの状態変化の検出
- **画像認識の重要性**:
  - キャラクターの現在の状態を正確に把握
  - 適切なアニメーション遷移の制御
  - 感情表現の精度向上
  - 自然な動作の実現

#### 3.2 既存技術
- **FishAudio**: 音声合成（.env設定済み）
- **OpenAI API**: チャット・TTS（.env設定済み）
- **Gemini API**: 画像分析（.env設定済み）

### 4. 制約事項

#### 4.1 技術的制約
- **Live2D Cubism Editor**: 無料版では.moc3ファイル書き出し不可
- **VTube Studio**: .moc3ファイルが必要
- **API制限**: 各AI技術のAPI制限・コスト制限
- **計算リソース**: 高品質動画生成の計算要求

#### 4.2 実装上の制約
- **統合の複雑性**: 複数AI技術の統合方法が不明
- **パフォーマンス最適化**: リアルタイム性と高品質の両立
- **エラーハンドリング**: 各AI技術のエラー処理
- **画像認識の必要性**: キャラクターを動かすには画像認識AIが必須
  - CLIPやGemini 2.5 Flashなしでは適切なアニメーション制御が不可能
  - 現在のキャラクター状態の把握ができない
  - 自然な動作遷移が実現できない

## 現在の状況

### 5. 完了済み項目
- [x] 現在のmain.pyと.envの状況を把握
- [x] yume_image.pngからLive2Dモデルを作成
- [x] VTube Studioとの連携を設定
- [x] Live2Dモデルの動作テスト
- [x] テクスチャの品質問題を分析
- [x] 会話議事録を作成
- [x] キャラがコメントから動作するデモを作成
- [x] VTube Studioの設定とLive2Dモデルの保存場所を記録
- [x] Live2D Cubism Editorの課金要件を分析
- [x] 最新AI技術を使ったVTube Studio代替案を設計

### 6. 問題点

#### 6.1 技術的問題
- **テクスチャ品質**: 既存テクスチャの品質が不十分
- **.moc3ファイル**: Live2D Cubism Editor PRO版が必要
- **VTube Studio連携**: 正しいモデル構造が必要

#### 6.2 実装上の問題
- **AI技術統合**: 具体的な統合方法が不明
- **リアルタイム性**: 高品質とリアルタイム性の両立
- **配信連携**: 配信プラットフォームとの連携方法

## 解決策

### 7. 選択肢

#### 7.1 既存モデル改造
- **方法**: akari_vtsをベースにyumeキャラに改造
- **メリット**: 既存の.moc3ファイルを活用
- **デメリット**: キャラクターの個性が制限される

#### 7.2 最新AI技術活用
- **方法**: VTube Studioを使わずにAI技術で直接動画生成
- **メリット**: 高品質、柔軟性、課金不要
- **デメリット**: 実装が複雑、VTube Studio連携不可

#### 7.3 課金してLive2D Cubism Editor PRO版使用
- **方法**: 月額2,288円でPRO版を使用
- **メリット**: 正しい.moc3ファイル生成可能
- **デメリット**: 課金が必要

### 8. 推奨案

**最新AI技術活用案**を推奨：

1. **yume_image.png**をベースにAI技術で動画生成
2. **コメント解析**で感情・アクション検出
3. **リアルタイム動画生成**でキャラクター動作
4. **配信プラットフォーム直接連携**で配信

## 次のステップ

### 9. 実装計画

#### 9.1 Phase 1: 基盤構築
- yume_image.pngの詳細分析
- AI技術の統合方法確立
- 基本的な動画生成システム構築

#### 9.2 Phase 2: コメント連携
- コメント解析システム実装
- 感情・アクション検出機能
- リアルタイム動画生成

#### 9.3 Phase 3: 配信統合
- 配信プラットフォーム連携
- 視聴者インタラクション機能
- パフォーマンス最適化

### 10. 成功基準

- **機能**: コメントからキャラクターが動作する
- **品質**: 高品質な動画生成
- **リアルタイム性**: 3秒以内の応答
- **配信**: TikTok/YouTube Liveで配信可能
- **安定性**: 長時間配信に対応

## 最新AI技術による再構築案

### 11. 声の仕組み（FishAudio + 最新AI技術）

#### 11.1 既存の声システム
- **FishAudio Voice ID**: `e32d8978e5b740058b87310599f15b4d`
- **感情対応**: happy, sad, laugh, surprised, calm, tender, serious, playful
- **音声品質**: 高品質なTTS、リップシンク対応

#### 11.2 最新AI技術での強化
- **Wan S2V**: 音声から映画品質の動画生成でリップシンク精度向上
- **EMO**: 写真/イラストからリアルな表情生成で音声と表情の同期
- **StreamDiffusion**: リアルタイム音声→表情生成パイプライン

### 12. 脳の仕組み（OpenAI + 最新AI技術）

#### 12.1 既存の脳システム
- **人格**: 「もも」という可愛い女の子
- **会話スタイル**: 親しみやすいタメ口、絵文字1つ、1-2文・最大80文字
- **感情表現**: 会話の状況に合わせた自然な感情表現
- **メモリ機能**: ユーザーの長期プロフィール記憶
- **自動発話**: アイドル時の自然な独り言

#### 12.2 最新AI技術での強化
- **LLM統合**: GPT-4o-mini + 最新のマルチモーダルLLM
- **感情理解**: CLIPによる画像・テキスト・音声の統合理解
- **リアルタイム学習**: ユーザーとの会話から継続的に学習・適応

### 13. 動かし方の仕組み（最新AI技術）

#### 13.1 既存の動作システム
- **コメント解析**: 感情・アクション検出
- **感情**: happy, sad, angry, surprised, love, excited
- **アクション**: wave, jump, nod, shake, blink, smile
- **リアルタイム制御**: VTube Studioパラメータ制御

#### 13.2 最新AI技術での代替案
- **PixAI i2v v3.0**: 高一貫性の画像→動画生成
- **RAIN**: リアルタイムアニメーション生成AI
- **MoCha**: 映画品質の人間動画生成
- **直接動画生成**: VTube Studioを使わずに直接動画ストリーム生成

### 14. 統合システム設計

#### 14.1 アーキテクチャ
```
ユーザーコメント → 感情解析 → 音声生成 → 動画生成 → 配信
     ↓              ↓         ↓         ↓
   LLM脳 → FishAudio声 → AI動画 → TikTok/YouTube
```

#### 14.2 技術スタック
- **脳**: OpenAI GPT-4o-mini + 最新マルチモーダルLLM
- **声**: FishAudio (既存) + Wan S2V (リップシンク強化)
- **動き**: PixAI i2v v3.0 + RAIN + MoCha
- **配信**: 直接動画ストリーム生成

### 15. 実装優先順位

#### 15.1 Phase 1: 既存システムの理解と強化
- FishAudio + OpenAI の既存システムを完全理解
- 感情解析とアクション検出の精度向上
- メモリ機能と自動発話の最適化

#### 15.2 Phase 2: 最新AI技術の統合
- PixAI i2v v3.0 による動画生成システム構築
- RAIN によるリアルタイムアニメーション
- 直接動画ストリーム生成の実装

#### 15.3 Phase 3: 配信統合と最適化
- TikTok/YouTube Live 直接連携
- パフォーマンス最適化
- 視聴者インタラクション機能

## リスク管理

### 16. 技術的リスク
- **AI技術の統合困難**: 代替技術の検討
- **パフォーマンス問題**: 最適化手法の実装
- **API制限**: 複数APIの併用

### 17. プロジェクトリスク
- **スケジュール遅延**: 段階的実装
- **品質問題**: 継続的な品質改善
- **コスト増加**: 予算管理の徹底
=======
# AITu2
GPUにつなぐために
>>>>>>> fc3e5032987bbcd1a1b764a9d4b6676bca28138e
